# Polydev SWE-bench: Multi-Model Ensemble for Automated Software Engineering

[![SWE-bench Verified](https://img.shields.io/badge/SWE--bench%20Verified-74.6%25-brightgreen)](https://www.swebench.com/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

## ğŸ¯ Key Finding

**Single-model and multi-model approaches solve DIFFERENT problems.** Combining them in a hybrid ensemble achieves **74.6% pass rate** (373/500) compared to 64.6% for baseline aloneâ€”a **15.5% relative improvement**.

## Overview

This repository contains the implementation and evaluation of a **hybrid ensemble approach** for automated software engineering on the [SWE-bench Verified](https://www.swebench.com/) benchmark. Our approach uses Claude Haiku 4.5 as the base model and optionally consults other models via [Polydev MCP](https://polydev.ai) for alternative perspectives.

## ğŸ“Š Results Summary

### Pass Rates (500 Instance Full Evaluation)

| Approach | Pass Rate | Instances Solved | Unique Solves |
|----------|-----------|------------------|---------------|
| Baseline (Claude Haiku 4.5) | 64.6% | 323/500 | 40 |
| Polydev (Multi-Model Consultation) | 66.6% | 333/500 | 50 |
| **Hybrid Ensemble** | **74.6%** | **373/500** | - |

### Complementarity Analysis

The approaches solve **different** problems (not redundantly):

| Category | Count | Description |
|----------|-------|-------------|
| Both solved | 283 | Core overlap - both approaches succeeded |
| **Baseline only** | 40 | Instances where single-model succeeded but multi-model failed |
| **Polydev only** | 50 | Instances where multi-model consultation provided the winning insight |

### Cost & Performance Metrics

| Metric | Baseline | Polydev | Hybrid |
|--------|----------|---------|--------|
| Total Cost | $1.98 | $4.88 | $6.86 |
| Avg Duration | 386s | 558s | - |
| Avg Turns | 59 | 68 | - |
| Est. Total Tokens | 989K | 1.98M | 2.97M |

## ğŸ”¬ Methodology

### Base Model Configuration
- **Model**: Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- **Thinking Budget**: 128,000 tokens (Anthropic methodology)
- **Max Turns**: 250
- **Prompt**: "Use tools 100+ times, implement tests first"

### Multi-Model Consultation (Polydev)
- **System**: [Polydev MCP](https://polydev.ai) (configured via dashboard)
- **Consultation Models**: GPT 5.2 Codex, Gemini 3 Flash Preview
- **Success Rate**: 97.5%
- **Avg Consultation Time**: 118s

### Hybrid Ensemble Strategy

```
Problem Statement â”€â”¬â”€â–º [Baseline Path] â”€â–º Patch A â”€â”
                   â”‚                               â”œâ”€â–º Test Validation â”€â–º Best Patch
                   â””â”€â–º [Polydev Path] â”€â”€â–º Patch B â”€â”˜
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/backspacevenkat/polydev-swe-bench.git
cd polydev-swe-bench

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
export ANTHROPIC_API_KEY="your-api-key"
```

### Running Evaluations

```bash
# Quick test (5 instances)
python scripts/swe_bench_baseline.py --mode test --workers 2

# Validation run (40 instances)
python scripts/swe_bench_baseline.py --mode validation --workers 4
python scripts/swe_bench_polydev.py --mode validation --workers 4

# Full evaluation (500 instances)
python scripts/swe_bench_baseline.py --mode full --workers 8
python scripts/swe_bench_polydev.py --mode full --workers 8
```

### Running SWE-bench Evaluation

```bash
# Install SWE-bench evaluation harness
pip install swebench

# Run evaluation
python -m swebench.harness.run_evaluation \
    -d princeton-nlp/SWE-bench_Verified \
    -s test \
    -p results/baseline/all_preds.jsonl \
    -id baseline-run
```

## ğŸ“ Repository Structure

```
polydev-swe-bench/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ PAPER_OUTLINE.md             # Research paper outline
â”œâ”€â”€ SWE_BENCH_SUBMISSION_GUIDE.md # Leaderboard submission guide
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ swe_bench_baseline.py    # Baseline evaluation script
â”‚   â”œâ”€â”€ swe_bench_polydev.py     # Polydev-enhanced evaluation
â”‚   â””â”€â”€ analyze_results.py       # Results analysis
â”‚
â”œâ”€â”€ agent_v3/
â”‚   â”œâ”€â”€ agent.py                 # Main agent implementation
â”‚   â”œâ”€â”€ model.py                 # Claude API wrapper
â”‚   â””â”€â”€ consultation.py          # Polydev consultation logic
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â”œâ”€â”€ all_preds.jsonl      # Baseline predictions
â”‚   â”‚   â”œâ”€â”€ metrics.jsonl        # Per-instance metrics
â”‚   â”‚   â””â”€â”€ trajectories/        # Reasoning traces
â”‚   â””â”€â”€ polydev/
â”‚       â”œâ”€â”€ all_preds.jsonl      # Polydev predictions
â”‚       â”œâ”€â”€ metrics.jsonl        # Per-instance metrics
â”‚       â””â”€â”€ trajectories/        # Reasoning traces
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ hybrid_ensemble.py       # Hybrid ensemble evaluation
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ METHODOLOGY.md           # Detailed methodology
    â””â”€â”€ REPRODUCIBILITY.md       # Reproduction instructions
```

## ğŸ”„ Reproducibility

### Experimental Setup
- **Benchmark**: SWE-bench Verified (500 instances, all evaluated)
- **Base Model**: Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
- **Thinking Budget**: 128,000 tokens
- **Hardware**: macOS (Darwin 24.5.0)
- **Date**: December 25-27, 2025
- **Total Evaluation Time**: ~48 hours

### Data Availability
- **Benchmark**: [SWE-bench Verified](https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified)
- **Predictions**: Available in `results/` directory
- **Trajectories**: Reasoning traces for each instance
- **Metrics**: Per-instance cost, time, and token usage

### Verification Steps
1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Set up API keys
4. Run: `python scripts/swe_bench_baseline.py --mode validation`
5. Compare results with `results/baseline/` directory

## ğŸ“ˆ Why Hybrid Ensemble Works

Our analysis reveals that baseline and Polydev solve **different types of problems**:

### Baseline-Only Wins
| Instance | Why Baseline Was Better |
|----------|------------------------|
| `django__django-11532` | Simple fix, extra context added noise |
| `astropy__astropy-14508` | Straightforward logic fix |
| `sympy__sympy-15976` | Direct pattern matching |

### Polydev-Only Wins
| Instance | Why Polydev Helped |
|----------|-------------------|
| `sympy__sympy-13031` | Alternative analysis approach |
| `pylint-dev__pylint-7080` | Multi-perspective identified edge case |
| `scikit-learn__scikit-learn-25973` | Feature selection insight |

## ğŸ“– Citation

If you use this work, please cite:

```bibtex
@article{polydev-swe-bench-2025,
  title={Ensemble Multi-Model Consultation for Automated Software Engineering:
         When Single Models Get Stuck, Diverse Perspectives Help},
  author={Venkat B.},
  year={2025},
  url={https://github.com/backspacevenkat/polydev-swe-bench}
}
```

## ğŸ”— Related Work

- [SWE-bench](https://www.swebench.com/) - The benchmark
- [SWE-agent](https://github.com/princeton-nlp/SWE-agent) - Agent-based approach
- [Agentless](https://github.com/OpenAutoCoder/Agentless) - Non-agent approach
- [Polydev](https://polydev.ai) - Multi-model consultation platform

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Anthropic for Claude API access
- Princeton NLP for SWE-bench benchmark
- Polydev team for multi-model consultation platform
