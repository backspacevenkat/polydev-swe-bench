# Polydev SWE-bench Evaluation Configuration
# ==========================================

# Experiment Settings
experiment:
  name: "polydev-swe-bench-verified"
  version: "1.0.0"
  description: "Multi-model consultation evaluation on SWE-bench Verified"

# Dataset Configuration
dataset:
  name: "swe-bench-verified"
  total_tasks: 500
  sample_size: 10  # For initial testing
  seed: 42  # For reproducibility

# Model Configuration
models:
  base:
    name: "claude-opus-4.5"
    provider: "anthropic"
    access: "claude-code-cli"
    cost_per_query: 0.0  # Free via CLI

  consultation:
    - name: "gpt-5.2"
      provider: "openai"
      access: "codex-cli"
      cost_per_query: 0.0  # Free via CLI

    - name: "gemini-3-pro"
      provider: "google"
      access: "polydev-mcp"
      cost_per_query: 0.001  # ~$0.001 via MCP

# Confidence Configuration
confidence:
  threshold: 8  # Consult if confidence < 8
  min_score: 1
  max_score: 10

# Consultation Settings
consultation:
  enabled: true
  models: ["gpt-5.2", "gemini-3-pro"]
  max_retries: 2
  timeout_seconds: 120

# Execution Settings
execution:
  max_concurrent_tasks: 1  # Sequential for reproducibility
  task_timeout_seconds: 600  # 10 minutes per task
  retry_on_failure: true
  max_retries: 2

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s %(levelname)s [%(name)s] %(message)s"
  log_to_file: true
  log_dir: "logs"

# Output Configuration
output:
  results_dir: "results"
  save_patches: true
  save_logs: true
  save_model_responses: true

# Monitoring
monitoring:
  track_latency: true
  track_costs: true
  track_consultations: true
  real_time_updates: true

# Docker Configuration (for patch testing)
docker:
  enabled: true
  image_prefix: "swebench"
  timeout_seconds: 900  # 15 minutes for test execution
  cleanup: true

# Statistical Analysis
statistics:
  significance_level: 0.05
  test_type: "mcnemar"  # McNemar's test for paired binary data
  confidence_interval: 0.95
