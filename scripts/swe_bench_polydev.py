#!/usr/bin/env python3
"""
SWE-bench Polydev-Enhanced Runner
==================================

Uses Polydev multi-model consultation PROACTIVELY for enhanced problem-solving.

CORRECT STRATEGY (Value Proposition):
1. For EVERY problem, FIRST consult Polydev to get multi-model perspectives
2. Use GPT-4, Claude, Gemini, Grok insights WHILE Claude solves the problem
3. Track token usage per model for cost analysis
4. Claude benefits from diverse AI perspectives from the start

This approach:
- Leverages multi-model AI consultation for EVERY problem
- Provides enhanced context BEFORE Claude starts working
- Demonstrates the true value of Polydev: better solutions through AI collaboration
- Tracks per-model token usage for detailed cost analysis

Usage:
    # Test on 20 samples with 5 workers
    python swe_bench_polydev.py --test 20 --workers 5

    # Full run
    python swe_bench_polydev.py --workers 10

    # Resume interrupted run
    python swe_bench_polydev.py --resume
"""

import json
import subprocess
import shutil
import time
import threading
import argparse
import os
import re
import asyncio
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from datasets import load_dataset

# =============================================================================
# CONFIGURATION
# =============================================================================
MODEL = "claude-haiku-4-5-20251001"
THINKING_TOKENS = 128000  # 128K thinking budget
MAX_TURNS = 250  # Allow more turns for complex issues
TIMEOUT = 3000  # 50 min timeout
MAX_RETRIES = 2  # Retry empty patches up to 2 times

# Output directories
BASE_OUTPUT_DIR = Path("/Users/venkat/Documents/polydev-swe-bench/results/polydev")
PREDICTIONS_FILE = BASE_OUTPUT_DIR / "all_preds.jsonl"
METRICS_FILE = BASE_OUTPUT_DIR / "metrics.jsonl"
PROGRESS_FILE = BASE_OUTPUT_DIR / "progress.json"
LOGS_DIR = BASE_OUTPUT_DIR / "logs"
TRAJS_DIR = BASE_OUTPUT_DIR / "trajs"
POLYDEV_DIR = BASE_OUTPUT_DIR / "polydev_responses"

# Git settings
GIT_RETRY_COUNT = 3
GIT_RETRY_DELAY = 10

# =============================================================================
# PROMPTS
# =============================================================================
ANTHROPIC_PROMPT_ADDITION = """You should use tools as much as possible, ideally more than 100 times. You should also implement your own tests first before attempting the problem."""

# Polydev consultation prompt - used for EVERY problem
POLYDEV_ANALYSIS_PROMPT = """You are helping solve a GitHub issue. Analyze this problem and provide expert guidance:

## Issue:
{problem}

## Repository: {repo}

Provide:
1. **Root Cause Analysis**: What's the most likely cause of this bug/issue?
2. **Key Files to Investigate**: Which specific files/modules are probably affected?
3. **Fix Strategy**: Step-by-step approach to implement the fix
4. **Code Pattern**: What kind of code change is needed (e.g., add validation, fix logic, handle edge case)?
5. **Testing Approach**: How to verify the fix works correctly
6. **Potential Pitfalls**: What edge cases or gotchas to watch for

Be specific, technical, and actionable. This guidance will help solve the issue correctly."""

# Task template with Polydev insights included from the START
TASK_TEMPLATE_WITH_POLYDEV = """<issue_description>
{problem}
</issue_description>

<multi_model_expert_analysis>
The following expert analysis was generated by consulting multiple AI models (GPT-4, Claude, Gemini, Grok) to provide diverse perspectives on this issue:

{polydev_insights}
</multi_model_expert_analysis>

<instructions>
The repository is cloned at: {repo_dir}

Your task: Fix the issue described above by modifying the source code.

IMPORTANT: {anthropic_prompt}

## Expert Guidance Available
The multi-model analysis above provides:
- Root cause analysis from multiple AI perspectives
- Key files that likely need modification
- Suggested fix strategies
- Potential pitfalls to avoid

USE THIS GUIDANCE while investigating and implementing your fix!

## Recommended Workflow:
1. **EXPLORE EXTENSIVELY**: Verify the expert analysis by exploring suggested files. Use bash commands extensively to understand the codebase.

2. **WRITE A TEST FIRST**: Create a test that reproduces the bug as described in the issue.

3. **IMPLEMENT THE FIX**: Apply the suggested fix strategies, considering all perspectives.

4. **VERIFY WITH YOUR TEST**: Run your test to confirm the fix works.

## Rules:
- MODIFY: Source code files only
- DO NOT MODIFY: Test files, setup.py, pyproject.toml, configuration files
- Make MINIMAL changes - the smallest fix that solves the issue
- Use tools extensively - explore before you fix
- Consider edge cases identified by the experts

When you've completed the fix, say: TASK_COMPLETE
</instructions>"""

# =============================================================================
# THREAD-SAFE STATISTICS
# =============================================================================
lock = threading.Lock()
polydev_semaphore = threading.Semaphore(1)  # Process one Polydev call at a time (sequential)
stats = {
    "run_id": "",
    "model": MODEL,
    "thinking_tokens": THINKING_TOKENS,
    "total": 0,
    "completed": 0,
    "patches_generated": 0,
    "errors": 0,
    "total_cost": 0.0,
    "total_input_tokens": 0,
    "total_output_tokens": 0,
    "total_turns": 0,
    "total_time_sec": 0,
    # Polydev tracking - now for EVERY instance
    "polydev_calls": 0,
    "polydev_successes": 0,
    "polydev_cost": 0.0,
    "polydev_time_sec": 0,
    # Per-model token tracking
    "model_tokens": {
        "gpt-4": {"input": 0, "output": 0, "cost": 0.0},
        "claude": {"input": 0, "output": 0, "cost": 0.0},
        "gemini": {"input": 0, "output": 0, "cost": 0.0},
        "grok": {"input": 0, "output": 0, "cost": 0.0},
    },
    "start_time": None,
    "instances": {}
}


def log_progress():
    """Log progress to file and console."""
    elapsed = time.time() - stats["start_time"] if stats["start_time"] else 0
    rate = stats["completed"] / (elapsed / 60) if elapsed > 0 else 0
    eta_mins = (stats["total"] - stats["completed"]) / rate if rate > 0 else 0
    avg_turns = stats["total_turns"] / max(1, stats["completed"])
    avg_time = stats["total_time_sec"] / max(1, stats["completed"])
    patch_rate = 100 * stats["patches_generated"] / max(1, stats["completed"])
    total_cost = stats["total_cost"] + stats["polydev_cost"]

    # Calculate per-model token totals
    model_token_summary = {}
    for model, tokens in stats["model_tokens"].items():
        total_tokens = tokens["input"] + tokens["output"]
        if total_tokens > 0:
            model_token_summary[model] = {
                "input": tokens["input"],
                "output": tokens["output"],
                "total": total_tokens,
                "cost": tokens["cost"]
            }

    progress = {
        "timestamp": datetime.now().isoformat(),
        "run_id": stats["run_id"],
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "completed": stats["completed"],
        "total": stats["total"],
        "patches_generated": stats["patches_generated"],
        "patch_rate_pct": round(patch_rate, 1),
        "errors": stats["errors"],
        "claude_cost_usd": round(stats["total_cost"], 4),
        "polydev_cost_usd": round(stats["polydev_cost"], 4),
        "total_cost_usd": round(total_cost, 4),
        "polydev_calls": stats["polydev_calls"],
        "polydev_successes": stats["polydev_successes"],
        "polydev_time_sec": round(stats["polydev_time_sec"], 1),
        "total_input_tokens": stats["total_input_tokens"],
        "total_output_tokens": stats["total_output_tokens"],
        "model_token_breakdown": model_token_summary,  # Per-model breakdown
        "avg_turns": round(avg_turns, 1),
        "avg_time_sec": round(avg_time, 1),
        "elapsed_mins": round(elapsed / 60, 1),
        "rate_per_min": round(rate, 2),
        "eta_mins": round(eta_mins, 1)
    }

    PROGRESS_FILE.write_text(json.dumps(progress, indent=2))

    print(f"\n{'='*80}")
    print(f"[{datetime.now().strftime('%H:%M:%S')}] POLYDEV-ENHANCED RUN PROGRESS (PROACTIVE)")
    print(f"  Run ID: {stats['run_id']}")
    print(f"  Completed: {stats['completed']}/{stats['total']} ({100*stats['completed']/max(1,stats['total']):.1f}%)")
    print(f"  Patches: {stats['patches_generated']} ({patch_rate:.1f}%) | Errors: {stats['errors']}")
    print(f"  Claude Cost: ${stats['total_cost']:.4f} | Polydev Cost: ${stats['polydev_cost']:.4f} | Total: ${total_cost:.4f}")
    print(f"  Polydev: {stats['polydev_calls']} calls ({stats['polydev_successes']} success) | Time: {stats['polydev_time_sec']:.0f}s")
    
    # Show per-model token breakdown
    if model_token_summary:
        print(f"  Per-Model Tokens:")
        for model, data in model_token_summary.items():
            print(f"    {model}: {data['input']:,} in / {data['output']:,} out = {data['total']:,} total (${data['cost']:.4f})")
    
    print(f"  Claude Tokens: {stats['total_input_tokens']:,} in / {stats['total_output_tokens']:,} out")
    print(f"  Avg: {avg_turns:.1f} turns, {avg_time:.0f}s per instance")
    print(f"  Rate: {rate:.2f}/min | ETA: {eta_mins:.0f} mins")
    print(f"{'='*80}\n")


def progress_reporter():
    """Report progress every 2 minutes."""
    while stats["completed"] < stats["total"]:
        time.sleep(120)
        if stats["completed"] < stats["total"]:
            log_progress()


def consult_polydev(problem: str, repo: str, instance_id: str) -> dict:
    """Consult Polydev for multi-model analysis - called for EVERY instance.
    
    Uses the Polydev MCP to get perspectives from GPT-4, Claude, Gemini, Grok.
    Tracks token usage per model for detailed cost analysis.
    
    Uses semaphore to limit concurrent calls (local CLIs can't handle many parallel calls).
    """
    polydev_file = POLYDEV_DIR / f"{instance_id}.json"

    result = {
        "success": False,
        "insights": "",
        "models_consulted": [],
        "model_responses": {},  # Individual responses per model
        "model_tokens": {  # Token tracking per model
            "gpt-4": {"input": 0, "output": 0, "cost": 0.0},
            "claude": {"input": 0, "output": 0, "cost": 0.0},
            "gemini": {"input": 0, "output": 0, "cost": 0.0},
            "grok": {"input": 0, "output": 0, "cost": 0.0},
        },
        "total_cost": 0.0,
        "duration_sec": 0,
        "error": None
    }

    start_time = time.time()

    # Use semaphore to limit concurrent Polydev calls
    with polydev_semaphore:
        try:
            # Truncate problem if too long (keep under 6000 chars for good context)
            problem_truncated = problem[:6000] if len(problem) > 6000 else problem

            prompt = POLYDEV_ANALYSIS_PROMPT.format(
                problem=problem_truncated,
                repo=repo
            )

            # Escape for JavaScript
            escaped_prompt = prompt.replace('\\', '\\\\').replace('`', '\\`').replace('$', '\\$')

            # Call Polydev via MCP execution - properly track tokens
            polydev_script = f"""
const {{ polydev }} = await import('/Users/venkat/mcp-execution/dist/index.js');

async function main() {{
    await polydev.initialize();

    const startTime = Date.now();
    const perspectives = await polydev.getPerspectives(`{escaped_prompt}`);
    const duration = Date.now() - startTime;

    // Handle the actual response format: [{{type: "text", text: "..."}}]
    let combinedText = "";
    if (Array.isArray(perspectives) && perspectives.length > 0) {{
        // Extract the text from the array format
        combinedText = perspectives[0]?.text || "";
    }} else if (typeof perspectives === 'object' && perspectives !== null) {{
        // Fallback: old format with model keys
        combinedText = Object.values(perspectives).filter(v => v).join("\\n\\n---\\n\\n");
    }} else if (typeof perspectives === 'string') {{
        combinedText = perspectives;
    }}

    // Parse model names and estimate tokens from the formatted text
    const modelMatches = combinedText.match(/\\*\\*Local CLI Response\\*\\* \\(([^)]+)\\)|\\*\\*([^*]+)\\*\\*/g) || [];
    const modelsFound = [];
    for (const match of modelMatches) {{
        const m = match.match(/\\(([^)]+)\\)/);
        if (m) modelsFound.push(m[1]);
    }}

    // Estimate tokens from combined text
    const inputTokens = Math.ceil(`{escaped_prompt}`.length / 4);
    const outputTokens = Math.ceil(combinedText.length / 3.5);
    
    // Rough cost estimate (combined for all models)
    const totalCost = ((inputTokens + outputTokens) / 1000) * 0.01;

    console.log(JSON.stringify({{
        success: combinedText.length > 50,
        combinedInsights: combinedText,
        modelsFound: modelsFound,
        inputTokens: inputTokens,
        outputTokens: outputTokens,
        totalCost: Math.round(totalCost * 10000) / 10000,
        duration: duration
    }}));
    process.exit(0);
}}

main().catch(e => {{ console.log(JSON.stringify({{success: false, error: e.message}})); process.exit(1); }});
"""

            # Write temp script
            temp_script = Path(f"/tmp/polydev_{instance_id}.mjs")
            temp_script.write_text(polydev_script)

            # Execute with proper environment and working directory
            proc = subprocess.run(
                ["node", "--experimental-vm-modules", str(temp_script)],
                capture_output=True,
                text=True,
                timeout=240,  # 4 min timeout for Polydev
                cwd="/Users/venkat/mcp-execution",  # Run from mcp-execution directory
                env={**os.environ, "NODE_PATH": "/Users/venkat/mcp-execution/node_modules"}
            )

            # Parse output - handle new format with combinedInsights
            output_lines = proc.stdout.strip().split('\n')
            for line in reversed(output_lines):
                if line.startswith('{'):
                    try:
                        output = json.loads(line)
                        if output.get("success"):
                            result["success"] = True
                            
                            # Get the combined insights text directly
                            result["insights"] = output.get("combinedInsights", "")
                            result["models_consulted"] = output.get("modelsFound", [])
                            result["total_cost"] = output.get("totalCost", 0.0)
                            
                            # Store token data
                            result["input_tokens"] = output.get("inputTokens", 0)
                            result["output_tokens"] = output.get("outputTokens", 0)
                            
                        else:
                            result["error"] = output.get("error", "Unknown Polydev error")
                        break
                    except json.JSONDecodeError:
                        continue

            if not result["success"] and not result["error"]:
                result["error"] = f"No valid Polydev output. Stderr: {proc.stderr[:500]}"

            # Cleanup temp script
            temp_script.unlink(missing_ok=True)

        except subprocess.TimeoutExpired:
            result["error"] = "Polydev timeout after 240 seconds"
        except Exception as e:
            result["error"] = str(e)[:500]

        result["duration_sec"] = round(time.time() - start_time, 1)

        # Save Polydev response with full detail
        with open(polydev_file, "w") as f:
            json.dump(result, f, indent=2)

        return result


def git_clone_with_retry(repo: str, repo_dir: Path, base_commit: str) -> bool:
    """Clone and checkout with retries. Uses shallow clone for speed."""
    for attempt in range(GIT_RETRY_COUNT):
        try:
            if repo_dir.exists():
                shutil.rmtree(repo_dir)
            repo_dir.parent.mkdir(parents=True, exist_ok=True)

            # Shallow clone (faster for large repos)
            clone = subprocess.run(
                ["git", "clone", "--quiet", "--depth=1", f"https://github.com/{repo}.git", str(repo_dir)],
                capture_output=True, timeout=300,
                env={**os.environ, "GIT_TERMINAL_PROMPT": "0"}
            )
            if clone.returncode != 0:
                raise Exception(f"Clone failed: {clone.stderr.decode()[:200]}")

            # Fetch the specific commit we need
            fetch = subprocess.run(
                ["git", "fetch", "--quiet", "--depth=1", "origin", base_commit],
                cwd=repo_dir, capture_output=True, timeout=300,
                env={**os.environ, "GIT_TERMINAL_PROMPT": "0"}
            )
            if fetch.returncode != 0:
                # If specific commit fetch fails, try fetching more history
                subprocess.run(
                    ["git", "fetch", "--quiet", "--unshallow"],
                    cwd=repo_dir, capture_output=True, timeout=600,
                    env={**os.environ, "GIT_TERMINAL_PROMPT": "0"}
                )

            # Checkout specific commit
            checkout = subprocess.run(
                ["git", "checkout", "--quiet", base_commit],
                cwd=repo_dir, capture_output=True, timeout=120
            )
            if checkout.returncode != 0:
                raise Exception(f"Checkout failed: {checkout.stderr.decode()[:200]}")

            return True

        except Exception as e:
            if attempt < GIT_RETRY_COUNT - 1:
                time.sleep(GIT_RETRY_DELAY * (attempt + 1))
            else:
                raise e

    return False


def parse_claude_output(stdout: str, stderr: str) -> dict:
    """Parse Claude CLI output to extract metrics."""
    result = {
        "turns": 0,
        "cost": 0.0,
        "input_tokens": 0,
        "output_tokens": 0
    }

    try:
        output = json.loads(stdout)
        result["turns"] = output.get("num_turns", 0)
        result["cost"] = output.get("total_cost_usd", 0)
        result["input_tokens"] = output.get("input_tokens", 0)
        result["output_tokens"] = output.get("output_tokens", 0)
    except json.JSONDecodeError:
        cost_match = re.search(r'\$(\d+\.?\d*)', stdout + stderr)
        if cost_match:
            result["cost"] = float(cost_match.group(1))
        turns_match = re.search(r'(\d+)\s*turns?', stdout + stderr, re.IGNORECASE)
        if turns_match:
            result["turns"] = int(turns_match.group(1))

    return result


def run_instance(instance: dict) -> dict:
    """Run Claude with PROACTIVE Polydev consultation.

    CORRECT Strategy (Value Proposition):
    1. FIRST: Consult Polydev for multi-model perspectives on EVERY problem
    2. Use insights from GPT-4, Claude, Gemini, Grok WHILE Claude solves
    3. Track tokens per model for cost analysis
    
    This demonstrates the true value of Polydev: better solutions through 
    multi-model AI collaboration from the START.
    """
    instance_id = instance["instance_id"]
    repo = instance["repo"]
    base_commit = instance["base_commit"]
    problem = instance["problem_statement"]

    repo_dir = Path(f"/tmp/swe_repos/{instance_id}")
    log_file = LOGS_DIR / f"{instance_id}.log"
    traj_file = TRAJS_DIR / f"{instance_id}.json"

    start_time = time.time()
    print(f"\nðŸ”„ Starting: {instance_id} (repo: {repo})", flush=True)

    result = {
        "instance_id": instance_id,
        "model_name_or_path": MODEL,
        "model_patch": "",
        "success": False,
        "error": None,
        "retries": 0,
        "polydev_used": True,  # Now ALWAYS True - Polydev on every instance
        "metrics": {
            "turns": 0,
            "cost_usd": 0.0,
            "input_tokens": 0,
            "output_tokens": 0,
            "duration_sec": 0,
            "git_clone_sec": 0,
            "polydev_sec": 0,
            "polydev_success": False,
            "polydev_models": [],
            "polydev_model_tokens": {},  # Per-model token tracking
            "polydev_cost": 0.0,
            "claude_sec": 0,
            "retries": 0
        }
    }

    # ==========================================================================
    # STEP 1: ALWAYS consult Polydev FIRST for multi-model perspectives
    # ==========================================================================
    print(f"   ðŸ”® Consulting Polydev (GPT-4, Claude, Gemini, Grok)...", flush=True)
    polydev_start = time.time()
    polydev_result = consult_polydev(problem, repo, instance_id)
    result["metrics"]["polydev_sec"] = round(time.time() - polydev_start, 1)
    result["metrics"]["polydev_success"] = polydev_result["success"]
    result["metrics"]["polydev_models"] = polydev_result.get("models_consulted", [])
    result["metrics"]["polydev_model_tokens"] = polydev_result.get("model_tokens", {})
    result["metrics"]["polydev_cost"] = polydev_result.get("total_cost", 0.0)

    # Update global stats for Polydev
    with lock:
        stats["polydev_calls"] += 1
        if polydev_result["success"]:
            stats["polydev_successes"] += 1
        stats["polydev_cost"] += result["metrics"]["polydev_cost"]
        stats["polydev_time_sec"] += result["metrics"]["polydev_sec"]
        
        # Update per-model token stats
        for model, tokens in polydev_result.get("model_tokens", {}).items():
            if model in stats["model_tokens"]:
                stats["model_tokens"][model]["input"] += tokens.get("input", 0)
                stats["model_tokens"][model]["output"] += tokens.get("output", 0)
                stats["model_tokens"][model]["cost"] += tokens.get("cost", 0.0)

    polydev_insights = polydev_result.get("insights", "")
    if polydev_result["success"]:
        model_count = len(polydev_result.get("models_consulted", []))
        print(f"   âœ… Polydev: {model_count} models consulted in {result['metrics']['polydev_sec']}s", flush=True)
    else:
        print(f"   âš ï¸ Polydev failed: {polydev_result.get('error', 'Unknown')[:50]}", flush=True)
        # Use fallback message if Polydev fails
        polydev_insights = "Multi-model analysis was unavailable. Proceed with standard analysis."

    # ==========================================================================
    # STEP 2: Clone repo and run Claude WITH Polydev insights
    # ==========================================================================
    for attempt in range(MAX_RETRIES + 1):
        try:
            # Clone repository
            print(f"   ðŸ“¥ Cloning repo...", flush=True)
            clone_start = time.time()
            git_clone_with_retry(repo, repo_dir, base_commit)
            clone_sec = round(time.time() - clone_start, 1)
            result["metrics"]["git_clone_sec"] = clone_sec
            print(f"   âœ… Cloned in {clone_sec}s", flush=True)

            # Build prompt WITH Polydev insights from the START
            prompt = TASK_TEMPLATE_WITH_POLYDEV.format(
                repo_dir=str(repo_dir),
                problem=problem,
                anthropic_prompt=ANTHROPIC_PROMPT_ADDITION,
                polydev_insights=polydev_insights
            )
            
            if attempt > 0:
                prompt += f"""

IMPORTANT: This is retry attempt {attempt + 1}.
The previous attempt did NOT produce a working patch.
Review the multi-model expert analysis above and try a different approach.
You MUST use the Edit tool to actually modify files!"""

            # Run Claude with extended thinking
            env = os.environ.copy()
            env["MAX_THINKING_TOKENS"] = str(THINKING_TOKENS)

            print(f"   ðŸ¤– Starting Claude (attempt {attempt + 1}) with Polydev insights...", flush=True)
            claude_start = time.time()
            claude = subprocess.run(
                [
                    "claude",
                    "--model", MODEL,
                    "--print",
                    "--output-format", "json",
                    "--dangerously-skip-permissions",
                    "--max-turns", str(MAX_TURNS),
                    "-p", prompt
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT,
                cwd=repo_dir,
                env=env
            )
            result["metrics"]["claude_sec"] += round(time.time() - claude_start, 1)

            # Save log
            log_mode = "w" if attempt == 0 else "a"
            with open(log_file, log_mode) as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"=== ATTEMPT {attempt + 1} ===\n")
                f.write(f"=== INSTANCE: {instance_id} ===\n")
                f.write(f"=== TIMESTAMP: {datetime.now().isoformat()} ===\n")
                f.write(f"=== POLYDEV MODELS: {result['metrics']['polydev_models']} ===\n")
                f.write(f"=== POLYDEV SUCCESS: {result['metrics']['polydev_success']} ===\n")
                f.write(f"=== POLYDEV INSIGHTS (first 3000 chars) ===\n{polydev_insights[:3000]}\n\n")
                f.write(f"=== STDOUT ===\n{claude.stdout}\n\n")
                f.write(f"=== STDERR ===\n{claude.stderr}\n")

            # Parse output for metrics
            parsed = parse_claude_output(claude.stdout, claude.stderr)
            result["metrics"]["turns"] += parsed["turns"]
            result["metrics"]["cost_usd"] += parsed["cost"]
            result["metrics"]["input_tokens"] += parsed["input_tokens"]
            result["metrics"]["output_tokens"] += parsed["output_tokens"]

            # Get patch via git diff
            diff = subprocess.run(
                ["git", "diff", "HEAD"],
                capture_output=True,
                text=True,
                cwd=repo_dir
            )

            patch = diff.stdout.strip()
            # Ensure patch ends with newline
            if patch and not patch.endswith("\n"):
                patch += "\n"

            if patch:
                result["model_patch"] = patch
                result["success"] = True
                result["retries"] = attempt
                result["metrics"]["retries"] = attempt
                print(f"   âœ… Patch generated on attempt {attempt + 1}", flush=True)
                break
            else:
                if attempt < MAX_RETRIES:
                    print(f"   âš ï¸ No patch on attempt {attempt + 1}, retrying...", flush=True)
                    shutil.rmtree(repo_dir, ignore_errors=True)
                    time.sleep(5)
                else:
                    result["error"] = f"No patch after {MAX_RETRIES + 1} attempts (with Polydev insights)"
                    result["retries"] = attempt
                    result["metrics"]["retries"] = attempt

        except subprocess.TimeoutExpired:
            result["error"] = f"Timeout after {TIMEOUT} seconds (attempt {attempt + 1})"
            if attempt < MAX_RETRIES:
                shutil.rmtree(repo_dir, ignore_errors=True)
                continue
            break

        except Exception as e:
            result["error"] = str(e)[:500]
            if attempt < MAX_RETRIES:
                shutil.rmtree(repo_dir, ignore_errors=True)
                continue
            break

    # Final cleanup and stats update
    result["metrics"]["duration_sec"] = round(time.time() - start_time, 1)

    # Save trajectory with full Polydev details
    trajectory = {
        "instance_id": instance_id,
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "polydev_used": True,  # Always true now
        "polydev_success": result["metrics"]["polydev_success"],
        "polydev_models": result["metrics"]["polydev_models"],
        "polydev_model_tokens": result["metrics"]["polydev_model_tokens"],
        "polydev_cost": result["metrics"]["polydev_cost"],
        "polydev_insights_length": len(polydev_insights),
        "retries": result["retries"],
        "metrics": result["metrics"],
        "patch_generated": result["success"],
        "patch_length": len(result["model_patch"]),
        "error": result.get("error"),
        "timestamp": datetime.now().isoformat()
    }
    with open(traj_file, "w") as f:
        json.dump(trajectory, f, indent=2)

    # Update global stats
    with lock:
        stats["completed"] += 1
        stats["total_cost"] += result["metrics"]["cost_usd"]
        stats["total_input_tokens"] += result["metrics"]["input_tokens"]
        stats["total_output_tokens"] += result["metrics"]["output_tokens"]
        stats["total_turns"] += result["metrics"]["turns"]
        stats["total_time_sec"] += result["metrics"]["duration_sec"]
        if result["success"]:
            stats["patches_generated"] += 1
        else:
            stats["errors"] += 1
        stats["instances"][instance_id] = {
            "status": "completed" if result["success"] else "failed",
            "success": result["success"],
            "polydev_success": result["metrics"]["polydev_success"],
            "polydev_models": result["metrics"]["polydev_models"],
            "polydev_model_tokens": result["metrics"]["polydev_model_tokens"],
            "retries": result["retries"],
            "metrics": result["metrics"]
        }

    # Cleanup
    shutil.rmtree(repo_dir, ignore_errors=True)

    return result


def load_completed_instances() -> set:
    """Load already completed instances for resume capability."""
    completed = set()
    if PREDICTIONS_FILE.exists():
        with open(PREDICTIONS_FILE) as f:
            for line in f:
                try:
                    pred = json.loads(line)
                    completed.add(pred["instance_id"])
                except:
                    pass
    return completed


def save_metadata():
    """Save metadata.yaml for leaderboard submission."""
    total_cost = stats["total_cost"] + stats["polydev_cost"]
    metadata = {
        "name": f"Polydev-Enhanced-{stats['run_id']}",
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "methodology": "Anthropic methodology + Polydev multi-model consultation",
        "polydev": {
            "description": "Multi-model AI consultation (GPT-4, Claude, Gemini, Grok)",
            "usage": "Every instance receives multi-model analysis before patch generation"
        },
        "oss": True,
        "date": datetime.now().strftime("%Y-%m-%d"),
        "stats": {
            "total_instances": stats["total"],
            "patches_generated": stats["patches_generated"],
            "polydev_calls": stats["polydev_calls"],
            "claude_cost_usd": round(stats["total_cost"], 2),
            "polydev_cost_usd": round(stats["polydev_cost"], 2),
            "total_cost_usd": round(total_cost, 2),
            "total_tokens": stats["total_input_tokens"] + stats["total_output_tokens"]
        }
    }

    metadata_file = BASE_OUTPUT_DIR / "metadata.yaml"
    import yaml
    with open(metadata_file, "w") as f:
        yaml.dump(metadata, f, default_flow_style=False)


def save_readme():
    """Save README.md for leaderboard submission."""
    total_cost = stats["total_cost"] + stats["polydev_cost"]
    elapsed = (time.time() - stats["start_time"]) / 60 if stats["start_time"] else 0

    readme = f"""# SWE-bench Polydev-Enhanced Submission

## Model
- **Name**: Claude Haiku 4.5 + Polydev Multi-Model Consultation
- **Model ID**: {MODEL}
- **Extended Thinking**: {THINKING_TOKENS:,} tokens

## Methodology
Anthropic's SWE-bench methodology enhanced with Polydev:
1. **Polydev Consultation**: Every instance receives analysis from GPT-4, Claude, Gemini, Grok
2. **Insights Synthesis**: Multi-model perspectives combined for enhanced context
3. **Enhanced Generation**: Claude uses synthesized insights for patch generation
4. Extended thinking with 128K token budget
5. Default sampling parameters

## Results
- **Total Instances**: {stats['total']}
- **Patches Generated**: {stats['patches_generated']} ({100*stats['patches_generated']/max(1,stats['total']):.1f}%)
- **Polydev Calls**: {stats['polydev_calls']}
- **Claude Cost**: ${stats['total_cost']:.2f}
- **Polydev Cost**: ${stats['polydev_cost']:.2f}
- **Total Cost**: ${total_cost:.2f}
- **Total Tokens**: {stats['total_input_tokens'] + stats['total_output_tokens']:,}

## Run Details
- **Run ID**: {stats['run_id']}
- **Date**: {datetime.now().strftime("%Y-%m-%d")}
- **Duration**: {elapsed:.0f} minutes

## Submission
```bash
sb-cli submit swe-bench_verified test --predictions_path all_preds.jsonl --run_id {stats['run_id']}
```
"""

    readme_file = BASE_OUTPUT_DIR / "README.md"
    readme_file.write_text(readme)


def main():
    parser = argparse.ArgumentParser(description="SWE-bench Polydev-Enhanced Runner")
    parser.add_argument("--workers", type=int, default=10, help="Number of parallel workers")
    parser.add_argument("--test", type=int, help="Test mode: run on N samples only")
    parser.add_argument("--resume", action="store_true", help="Resume from previous run")
    parser.add_argument("--run-id", type=str, help="Custom run ID")
    parser.add_argument("--instances", type=str, help="JSON file with list of instance IDs to run")
    args = parser.parse_args()

    # Setup
    run_id = args.run_id or f"polydev-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    stats["run_id"] = run_id

    print("="*80)
    print("SWE-BENCH POLYDEV-ENHANCED RUNNER")
    print("PROACTIVE Multi-Model AI Consultation")
    print("="*80)
    print(f"  Run ID: {run_id}")
    print(f"  Model: {MODEL}")
    print(f"  Thinking Budget: {THINKING_TOKENS:,} tokens")
    print(f"  Workers: {args.workers}")
    print(f"  Polydev: PROACTIVE (consults GPT-4, Claude, Gemini, Grok for EVERY instance)")
    if args.test:
        print(f"  TEST MODE: {args.test} samples only")
    if args.resume:
        print(f"  RESUME MODE: Continuing from previous run")
    print("="*80)

    # Create output directories
    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(exist_ok=True)
    TRAJS_DIR.mkdir(exist_ok=True)
    POLYDEV_DIR.mkdir(exist_ok=True)

    # Load dataset
    print("\nLoading SWE-bench Verified dataset...")
    ds = load_dataset("princeton-nlp/SWE-bench_Verified", split="test")
    all_instances = list(ds)

    # Handle resume
    completed_instances = set()
    if args.resume:
        completed_instances = load_completed_instances()
        print(f"  Found {len(completed_instances)} already completed instances")

    # Filter instances
    instances = [inst for inst in all_instances if inst["instance_id"] not in completed_instances]

    # Filter by specific instance IDs if provided
    if args.instances:
        with open(args.instances) as f:
            target_ids = set(json.load(f))
        instances = [inst for inst in instances if inst["instance_id"] in target_ids]
        print(f"  Filtered to {len(instances)} specific instances from {args.instances}")

    # Test mode
    if args.test:
        instances = instances[:args.test]

    stats["total"] = len(instances)
    stats["start_time"] = time.time()

    print(f"\nInstances to process: {len(instances)}")
    print("Starting parallel execution with Polydev consultation...")
    print("="*80 + "\n")

    # Start progress reporter
    reporter = threading.Thread(target=progress_reporter, daemon=True)
    reporter.start()

    # Run in parallel
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = {executor.submit(run_instance, inst): inst for inst in instances}

        for future in as_completed(futures):
            result = future.result()

            # Save prediction (leaderboard format)
            with lock:
                with open(PREDICTIONS_FILE, "a") as f:
                    pred = {
                        "instance_id": result["instance_id"],
                        "model_name_or_path": result["model_name_or_path"],
                        "model_patch": result["model_patch"]
                    }
                    f.write(json.dumps(pred) + "\n")

                # Save metrics
                with open(METRICS_FILE, "a") as f:
                    metrics_entry = {
                        "instance_id": result["instance_id"],
                        "success": result["success"],
                        "metrics": result["metrics"],
                        "error": result.get("error"),
                        "timestamp": datetime.now().isoformat()
                    }
                    f.write(json.dumps(metrics_entry) + "\n")

            # Print status - Polydev is ALWAYS invoked now (PROACTIVE strategy)
            status = "âœ“" if result["success"] else "âœ—"
            pd_status = "P" if result["metrics"].get("polydev_success") else "p"  # P=success, p=failed
            err = f" [{result.get('error', '')[:25]}]" if result.get("error") else ""
            m = result["metrics"]
            models_str = ",".join(m.get("polydev_models", [])[:2]) if m.get("polydev_models") else "none"
            patch_rate = 100 * stats["patches_generated"] / max(1, stats["completed"])
            print(f"[{stats['completed']}/{stats['total']}] {status}{pd_status} {result['instance_id'][:40]:<40} "
                  f"t:{m['turns']:>3} ${m['cost_usd']:.3f} {m['duration_sec']:>4.0f}s "
                  f"[{models_str}] [{stats['patches_generated']} ({patch_rate:.0f}%)]{err}")

    # Final summary
    log_progress()

    # Save metadata and README
    try:
        save_metadata()
        save_readme()
    except Exception as e:
        print(f"Warning: Could not save metadata/readme: {e}")

    total_cost = stats["total_cost"] + stats["polydev_cost"]
    print(f"\n{'='*80}")
    print("POLYDEV-ENHANCED RUN COMPLETE!")
    print(f"{'='*80}")
    print(f"  Predictions: {PREDICTIONS_FILE}")
    print(f"  Metrics: {METRICS_FILE}")
    print(f"  Polydev Responses: {POLYDEV_DIR}")
    print(f"\n  Patches: {stats['patches_generated']}/{stats['total']} ({100*stats['patches_generated']/max(1,stats['total']):.1f}%)")
    print(f"  Polydev Calls: {stats['polydev_calls']}")
    print(f"  Claude Cost: ${stats['total_cost']:.4f}")
    print(f"  Polydev Cost: ${stats['polydev_cost']:.4f}")
    print(f"  Total Cost: ${total_cost:.4f}")
    print(f"\n  To submit to leaderboard:")
    print(f"  sb-cli submit swe-bench_verified test --predictions_path {PREDICTIONS_FILE} --run_id {run_id}")
    print("="*80)


if __name__ == "__main__":
    main()
