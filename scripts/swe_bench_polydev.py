#!/usr/bin/env python3
"""
SWE-bench Polydev-Enhanced Runner
==================================

Uses Polydev multi-model consultation GENEROUSLY for improved results.

Strategy:
1. For EVERY instance, consult Polydev to get multi-model analysis
2. Synthesize insights from GPT-4, Claude, Gemini, Grok
3. Provide enhanced context to Claude for patch generation
4. Track all Polydev-specific metrics for paper
5. RETRY LOGIC for empty patches (ensures 100% patch generation)

This should improve resolution rate from ~73% (baseline) to ~76%+

Usage:
    # Test on 20 samples with 5 workers
    python swe_bench_polydev.py --test 20 --workers 5

    # Full run
    python swe_bench_polydev.py --workers 10

    # Resume interrupted run
    python swe_bench_polydev.py --resume
"""

import json
import subprocess
import shutil
import time
import threading
import argparse
import os
import re
import asyncio
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from datasets import load_dataset

# =============================================================================
# CONFIGURATION
# =============================================================================
MODEL = "claude-haiku-4-5-20251001"
THINKING_TOKENS = 128000  # 128K thinking budget
MAX_TURNS = 250  # Allow more turns for complex issues
TIMEOUT = 3000  # 50 min timeout (extra time for Polydev + retries)
MAX_RETRIES = 2  # Retry empty patches up to 2 times

# Output directories
BASE_OUTPUT_DIR = Path("/Users/venkat/Documents/polydev-swe-bench/results/polydev")
PREDICTIONS_FILE = BASE_OUTPUT_DIR / "all_preds.jsonl"
METRICS_FILE = BASE_OUTPUT_DIR / "metrics.jsonl"
PROGRESS_FILE = BASE_OUTPUT_DIR / "progress.json"
LOGS_DIR = BASE_OUTPUT_DIR / "logs"
TRAJS_DIR = BASE_OUTPUT_DIR / "trajs"
POLYDEV_DIR = BASE_OUTPUT_DIR / "polydev_responses"

# Git settings
GIT_RETRY_COUNT = 3
GIT_RETRY_DELAY = 10

# =============================================================================
# PROMPTS
# =============================================================================
ANTHROPIC_PROMPT_ADDITION = """You should use tools as much as possible, ideally more than 100 times. You should also implement your own tests first before attempting the problem."""

# Polydev consultation prompt
POLYDEV_ANALYSIS_PROMPT = """Analyze this GitHub issue for a code fix. Provide:

1. **Root Cause Analysis**: What's likely causing this bug?
2. **Key Files**: Which files probably need modification?
3. **Fix Strategy**: What approach would fix this?
4. **Edge Cases**: What edge cases should be considered?
5. **Testing**: How should the fix be verified?

## Issue:
{problem}

## Repository: {repo}

Be specific and technical. Focus on actionable insights."""

# Enhanced task template with Polydev insights
TASK_TEMPLATE_POLYDEV = """<issue_description>
{problem}
</issue_description>

<expert_analysis>
The following analysis was generated by consulting multiple AI models (GPT-4, Claude, Gemini, Grok):

{polydev_insights}
</expert_analysis>

<instructions>
The repository is cloned at: {repo_dir}

Your task: Fix the issue described above by modifying the source code.

IMPORTANT: {anthropic_prompt}

Use the expert analysis above to guide your investigation. The analysis provides:
- Likely root cause of the bug
- Key files that probably need modification
- Suggested fix strategy
- Edge cases to consider

## Recommended Workflow:
1. **EXPLORE EXTENSIVELY**: Start by verifying the expert analysis. Use bash commands to explore the suggested files and confirm the root cause.

2. **WRITE A TEST FIRST**: Create a test that reproduces the bug as described in the issue.

3. **IMPLEMENT THE FIX**: Apply the suggested fix strategy, handling the identified edge cases.

4. **VERIFY WITH YOUR TEST**: Run your test to confirm the fix works.

## Rules:
- MODIFY: Source code files only
- DO NOT MODIFY: Test files, setup.py, pyproject.toml, configuration files
- Make MINIMAL changes - the smallest fix that solves the issue
- Use tools extensively - explore before you fix

When you've completed the fix, say: TASK_COMPLETE
</instructions>"""

# =============================================================================
# THREAD-SAFE STATISTICS
# =============================================================================
lock = threading.Lock()
stats = {
    "run_id": "",
    "model": MODEL,
    "thinking_tokens": THINKING_TOKENS,
    "total": 0,
    "completed": 0,
    "patches_generated": 0,
    "errors": 0,
    "total_cost": 0.0,
    "total_input_tokens": 0,
    "total_output_tokens": 0,
    "total_turns": 0,
    "total_time_sec": 0,
    "polydev_calls": 0,
    "polydev_cost": 0.0,
    "polydev_time_sec": 0,
    "start_time": None,
    "instances": {}
}


def log_progress():
    """Log progress to file and console."""
    elapsed = time.time() - stats["start_time"] if stats["start_time"] else 0
    rate = stats["completed"] / (elapsed / 60) if elapsed > 0 else 0
    eta_mins = (stats["total"] - stats["completed"]) / rate if rate > 0 else 0
    avg_turns = stats["total_turns"] / max(1, stats["completed"])
    avg_time = stats["total_time_sec"] / max(1, stats["completed"])
    patch_rate = 100 * stats["patches_generated"] / max(1, stats["completed"])
    total_cost = stats["total_cost"] + stats["polydev_cost"]

    progress = {
        "timestamp": datetime.now().isoformat(),
        "run_id": stats["run_id"],
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "completed": stats["completed"],
        "total": stats["total"],
        "patches_generated": stats["patches_generated"],
        "patch_rate_pct": round(patch_rate, 1),
        "errors": stats["errors"],
        "claude_cost_usd": round(stats["total_cost"], 4),
        "polydev_cost_usd": round(stats["polydev_cost"], 4),
        "total_cost_usd": round(total_cost, 4),
        "polydev_calls": stats["polydev_calls"],
        "polydev_time_sec": round(stats["polydev_time_sec"], 1),
        "total_input_tokens": stats["total_input_tokens"],
        "total_output_tokens": stats["total_output_tokens"],
        "avg_turns": round(avg_turns, 1),
        "avg_time_sec": round(avg_time, 1),
        "elapsed_mins": round(elapsed / 60, 1),
        "rate_per_min": round(rate, 2),
        "eta_mins": round(eta_mins, 1)
    }

    PROGRESS_FILE.write_text(json.dumps(progress, indent=2))

    print(f"\n{'='*80}")
    print(f"[{datetime.now().strftime('%H:%M:%S')}] POLYDEV-ENHANCED RUN PROGRESS")
    print(f"  Run ID: {stats['run_id']}")
    print(f"  Completed: {stats['completed']}/{stats['total']} ({100*stats['completed']/max(1,stats['total']):.1f}%)")
    print(f"  Patches: {stats['patches_generated']} ({patch_rate:.1f}%) | Errors: {stats['errors']}")
    print(f"  Claude Cost: ${stats['total_cost']:.4f} | Polydev Cost: ${stats['polydev_cost']:.4f} | Total: ${total_cost:.4f}")
    print(f"  Polydev Calls: {stats['polydev_calls']} | Polydev Time: {stats['polydev_time_sec']:.0f}s")
    print(f"  Tokens: {stats['total_input_tokens']:,} in / {stats['total_output_tokens']:,} out")
    print(f"  Avg: {avg_turns:.1f} turns, {avg_time:.0f}s per instance")
    print(f"  Rate: {rate:.2f}/min | ETA: {eta_mins:.0f} mins")
    print(f"{'='*80}\n")


def progress_reporter():
    """Report progress every 2 minutes."""
    while stats["completed"] < stats["total"]:
        time.sleep(120)
        if stats["completed"] < stats["total"]:
            log_progress()


def consult_polydev(problem: str, repo: str, instance_id: str) -> dict:
    """Consult Polydev for multi-model analysis."""
    polydev_file = POLYDEV_DIR / f"{instance_id}.json"

    result = {
        "success": False,
        "insights": "",
        "models_consulted": [],
        "cost": 0.0,
        "duration_sec": 0,
        "error": None
    }

    start_time = time.time()

    try:
        # Truncate problem if too long (keep under 4000 chars for efficiency)
        problem_truncated = problem[:4000] if len(problem) > 4000 else problem

        prompt = POLYDEV_ANALYSIS_PROMPT.format(
            problem=problem_truncated,
            repo=repo
        )

        # Escape backticks for JavaScript template literal (must be outside f-string)
        escaped_prompt = prompt.replace('`', '\\`')

        # Call Polydev via node script
        # Using the MCP execution approach
        polydev_script = f"""
import {{ polydev }} from '/Users/venkat/mcp-execution/dist/index.js';

async function main() {{
    await polydev.initialize();

    const perspectives = await polydev.getPerspectives(`{escaped_prompt}`);

    console.log(JSON.stringify({{
        success: true,
        perspectives: perspectives,
        models: ['gpt-4', 'claude', 'gemini', 'grok']
    }}));
}}

main().catch(e => console.log(JSON.stringify({{success: false, error: e.message}})));
"""

        # Write temp script
        temp_script = Path(f"/tmp/polydev_{instance_id}.mjs")
        temp_script.write_text(polydev_script)

        # Execute
        proc = subprocess.run(
            ["node", str(temp_script)],
            capture_output=True,
            text=True,
            timeout=120,  # 2 min timeout for Polydev
            env={**os.environ, "NODE_PATH": "/Users/venkat/mcp-execution/node_modules"}
        )

        # Parse output
        if proc.stdout.strip():
            try:
                output = json.loads(proc.stdout.strip().split('\n')[-1])
                if output.get("success"):
                    result["success"] = True
                    perspectives = output.get("perspectives", {})

                    # Format insights from all models
                    insights_parts = []
                    for model, response in perspectives.items():
                        if response:
                            result["models_consulted"].append(model)
                            insights_parts.append(f"### {model.upper()} Analysis:\n{response[:1000]}")

                    result["insights"] = "\n\n".join(insights_parts)
                    result["cost"] = 0.01  # Estimated cost per Polydev call
                else:
                    result["error"] = output.get("error", "Unknown Polydev error")
            except json.JSONDecodeError as e:
                result["error"] = f"Failed to parse Polydev output: {e}"
        else:
            result["error"] = f"No Polydev output. Stderr: {proc.stderr[:500]}"

        # Cleanup temp script
        temp_script.unlink(missing_ok=True)

    except subprocess.TimeoutExpired:
        result["error"] = "Polydev timeout after 120 seconds"
    except Exception as e:
        result["error"] = str(e)[:500]

    result["duration_sec"] = round(time.time() - start_time, 1)

    # Save Polydev response
    with open(polydev_file, "w") as f:
        json.dump(result, f, indent=2)

    return result


def git_clone_with_retry(repo: str, repo_dir: Path, base_commit: str) -> bool:
    """Clone and checkout with retries."""
    for attempt in range(GIT_RETRY_COUNT):
        try:
            if repo_dir.exists():
                shutil.rmtree(repo_dir)
            repo_dir.parent.mkdir(parents=True, exist_ok=True)

            clone = subprocess.run(
                ["git", "clone", "--quiet", f"https://github.com/{repo}.git", str(repo_dir)],
                capture_output=True, timeout=600,
                env={**os.environ, "GIT_TERMINAL_PROMPT": "0"}
            )
            if clone.returncode != 0:
                raise Exception(f"Clone failed: {clone.stderr.decode()[:200]}")

            checkout = subprocess.run(
                ["git", "checkout", "--quiet", base_commit],
                cwd=repo_dir, capture_output=True, timeout=120
            )
            if checkout.returncode != 0:
                subprocess.run(
                    ["git", "fetch", "--quiet", "--depth=100", "origin"],
                    cwd=repo_dir, capture_output=True, timeout=300,
                    env={**os.environ, "GIT_TERMINAL_PROMPT": "0"}
                )
                checkout = subprocess.run(
                    ["git", "checkout", "--quiet", base_commit],
                    cwd=repo_dir, capture_output=True, timeout=120
                )
                if checkout.returncode != 0:
                    raise Exception(f"Checkout failed: {checkout.stderr.decode()[:200]}")

            return True

        except Exception as e:
            if attempt < GIT_RETRY_COUNT - 1:
                time.sleep(GIT_RETRY_DELAY * (attempt + 1))
            else:
                raise e

    return False


def parse_claude_output(stdout: str, stderr: str) -> dict:
    """Parse Claude CLI output to extract metrics."""
    result = {
        "turns": 0,
        "cost": 0.0,
        "input_tokens": 0,
        "output_tokens": 0
    }

    try:
        output = json.loads(stdout)
        result["turns"] = output.get("num_turns", 0)
        result["cost"] = output.get("total_cost_usd", 0)
        result["input_tokens"] = output.get("input_tokens", 0)
        result["output_tokens"] = output.get("output_tokens", 0)
    except json.JSONDecodeError:
        cost_match = re.search(r'\$(\d+\.?\d*)', stdout + stderr)
        if cost_match:
            result["cost"] = float(cost_match.group(1))
        turns_match = re.search(r'(\d+)\s*turns?', stdout + stderr, re.IGNORECASE)
        if turns_match:
            result["turns"] = int(turns_match.group(1))

    return result


def run_instance(instance: dict) -> dict:
    """Run Polydev consultation + Claude with extended thinking.
    
    Includes RETRY LOGIC: If no patch is generated, retries up to MAX_RETRIES times.
    """
    instance_id = instance["instance_id"]
    repo = instance["repo"]
    base_commit = instance["base_commit"]
    problem = instance["problem_statement"]

    repo_dir = Path(f"/tmp/swe_repos/{instance_id}")
    log_file = LOGS_DIR / f"{instance_id}.log"
    traj_file = TRAJS_DIR / f"{instance_id}.json"

    start_time = time.time()

    result = {
        "instance_id": instance_id,
        "model_name_or_path": MODEL,
        "model_patch": "",
        "success": False,
        "error": None,
        "retries": 0,
        "metrics": {
            "turns": 0,
            "cost_usd": 0.0,
            "input_tokens": 0,
            "output_tokens": 0,
            "duration_sec": 0,
            "git_clone_sec": 0,
            "polydev_sec": 0,
            "claude_sec": 0,
            "polydev_success": False,
            "polydev_models": [],
            "retries": 0
        }
    }

    # Step 1: Consult Polydev for multi-model analysis (only once, before retries)
    polydev_start = time.time()
    polydev_result = consult_polydev(problem, repo, instance_id)
    result["metrics"]["polydev_sec"] = round(time.time() - polydev_start, 1)
    result["metrics"]["polydev_success"] = polydev_result["success"]
    result["metrics"]["polydev_models"] = polydev_result.get("models_consulted", [])

    with lock:
        stats["polydev_calls"] += 1
        stats["polydev_cost"] += polydev_result.get("cost", 0)
        stats["polydev_time_sec"] += result["metrics"]["polydev_sec"]

    # Get insights (or fallback message)
    polydev_insights = polydev_result.get("insights", "")
    if not polydev_insights:
        polydev_insights = "Multi-model analysis was not available. Proceed with standard analysis."

    # Retry loop for empty patches
    for attempt in range(MAX_RETRIES + 1):
        try:
            # Clone repository (fresh clone for each retry)
            clone_start = time.time()
            git_clone_with_retry(repo, repo_dir, base_commit)
            result["metrics"]["git_clone_sec"] = round(time.time() - clone_start, 1)

            # Build enhanced prompt with Polydev insights - add retry context if this is a retry
            retry_context = ""
            if attempt > 0:
                retry_context = f"""

IMPORTANT: This is retry attempt {attempt + 1}. The previous attempt did NOT produce a working patch.
You MUST use the Edit tool to actually modify files. Do not just describe changes - make them!
After making changes, verify with 'git diff' that your changes are saved."""

            prompt = TASK_TEMPLATE_POLYDEV.format(
                repo_dir=str(repo_dir),
                problem=problem,
                anthropic_prompt=ANTHROPIC_PROMPT_ADDITION,
                polydev_insights=polydev_insights
            ) + retry_context

            # Run Claude with extended thinking
            env = os.environ.copy()
            env["MAX_THINKING_TOKENS"] = str(THINKING_TOKENS)

            claude_start = time.time()
            claude = subprocess.run(
                [
                    "claude",
                    "--model", MODEL,
                    "--print",
                    "--output-format", "json",
                    "--dangerously-skip-permissions",
                    "--max-turns", str(MAX_TURNS),
                    "--add-dir", str(repo_dir),
                    "-p", prompt
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT,
                cwd=repo_dir,
                env=env
            )
            result["metrics"]["claude_sec"] += round(time.time() - claude_start, 1)

            # Save log (append for retries)
            log_mode = "w" if attempt == 0 else "a"
            with open(log_file, log_mode) as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"=== ATTEMPT {attempt + 1} ===\n")
                f.write(f"=== INSTANCE: {instance_id} ===\n")
                f.write(f"=== TIMESTAMP: {datetime.now().isoformat()} ===\n")
                f.write(f"=== POLYDEV INSIGHTS ===\n{polydev_insights[:2000]}\n\n")
                f.write(f"=== STDOUT ===\n{claude.stdout}\n\n")
                f.write(f"=== STDERR ===\n{claude.stderr}\n")

            # Parse output for metrics (accumulate across retries)
            parsed = parse_claude_output(claude.stdout, claude.stderr)
            result["metrics"]["turns"] += parsed["turns"]
            result["metrics"]["cost_usd"] += parsed["cost"]
            result["metrics"]["input_tokens"] += parsed["input_tokens"]
            result["metrics"]["output_tokens"] += parsed["output_tokens"]

            # Get patch via git diff
            diff = subprocess.run(
                ["git", "diff", "HEAD"],
                capture_output=True,
                text=True,
                cwd=repo_dir
            )
            
            patch = diff.stdout.strip()
            
            # Check if we got a valid patch
            if patch:
                result["model_patch"] = patch
                result["success"] = True
                result["retries"] = attempt
                result["metrics"]["retries"] = attempt
                break  # Success! Exit retry loop
            else:
                # No patch generated
                if attempt < MAX_RETRIES:
                    print(f"  ⚠️  {instance_id}: Empty patch on attempt {attempt + 1}, retrying...")
                    # Clean up for retry
                    shutil.rmtree(repo_dir, ignore_errors=True)
                    time.sleep(5)  # Brief pause before retry
                else:
                    # All retries exhausted
                    result["error"] = f"No patch generated after {MAX_RETRIES + 1} attempts"
                    result["retries"] = attempt
                    result["metrics"]["retries"] = attempt

        except subprocess.TimeoutExpired:
            result["error"] = f"Timeout after {TIMEOUT} seconds (attempt {attempt + 1})"
            if attempt < MAX_RETRIES:
                shutil.rmtree(repo_dir, ignore_errors=True)
                continue
            break

        except Exception as e:
            result["error"] = str(e)[:500]
            if attempt < MAX_RETRIES:
                shutil.rmtree(repo_dir, ignore_errors=True)
                continue
            break

    # Final cleanup and stats update
    result["metrics"]["duration_sec"] = round(time.time() - start_time, 1)

    # Save trajectory
    trajectory = {
        "instance_id": instance_id,
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "polydev_used": True,
        "polydev_success": polydev_result["success"],
        "polydev_models": polydev_result.get("models_consulted", []),
        "polydev_insights_length": len(polydev_insights),
        "retries": result["retries"],
        "metrics": result["metrics"],
        "patch_generated": result["success"],
        "patch_length": len(result["model_patch"]),
        "error": result.get("error"),
        "timestamp": datetime.now().isoformat()
    }
    with open(traj_file, "w") as f:
        json.dump(trajectory, f, indent=2)

    # Update global stats
    with lock:
        stats["completed"] += 1
        stats["total_cost"] += result["metrics"]["cost_usd"]
        stats["total_input_tokens"] += result["metrics"]["input_tokens"]
        stats["total_output_tokens"] += result["metrics"]["output_tokens"]
        stats["total_turns"] += result["metrics"]["turns"]
        stats["total_time_sec"] += result["metrics"]["duration_sec"]
        if result["success"]:
            stats["patches_generated"] += 1
        else:
            stats["errors"] += 1
        stats["instances"][instance_id] = {
            "status": "completed" if result["success"] else "failed",
            "success": result["success"],
            "polydev_success": polydev_result["success"],
            "retries": result["retries"],
            "metrics": result["metrics"]
        }

    # Cleanup
    shutil.rmtree(repo_dir, ignore_errors=True)

    return result


def load_completed_instances() -> set:
    """Load already completed instances for resume capability."""
    completed = set()
    if PREDICTIONS_FILE.exists():
        with open(PREDICTIONS_FILE) as f:
            for line in f:
                try:
                    pred = json.loads(line)
                    completed.add(pred["instance_id"])
                except:
                    pass
    return completed


def save_metadata():
    """Save metadata.yaml for leaderboard submission."""
    total_cost = stats["total_cost"] + stats["polydev_cost"]
    metadata = {
        "name": f"Polydev-Enhanced-{stats['run_id']}",
        "model": MODEL,
        "thinking_tokens": THINKING_TOKENS,
        "methodology": "Anthropic methodology + Polydev multi-model consultation",
        "polydev": {
            "description": "Multi-model AI consultation (GPT-4, Claude, Gemini, Grok)",
            "usage": "Every instance receives multi-model analysis before patch generation"
        },
        "oss": True,
        "date": datetime.now().strftime("%Y-%m-%d"),
        "stats": {
            "total_instances": stats["total"],
            "patches_generated": stats["patches_generated"],
            "polydev_calls": stats["polydev_calls"],
            "claude_cost_usd": round(stats["total_cost"], 2),
            "polydev_cost_usd": round(stats["polydev_cost"], 2),
            "total_cost_usd": round(total_cost, 2),
            "total_tokens": stats["total_input_tokens"] + stats["total_output_tokens"]
        }
    }

    metadata_file = BASE_OUTPUT_DIR / "metadata.yaml"
    import yaml
    with open(metadata_file, "w") as f:
        yaml.dump(metadata, f, default_flow_style=False)


def save_readme():
    """Save README.md for leaderboard submission."""
    total_cost = stats["total_cost"] + stats["polydev_cost"]
    elapsed = (time.time() - stats["start_time"]) / 60 if stats["start_time"] else 0

    readme = f"""# SWE-bench Polydev-Enhanced Submission

## Model
- **Name**: Claude Haiku 4.5 + Polydev Multi-Model Consultation
- **Model ID**: {MODEL}
- **Extended Thinking**: {THINKING_TOKENS:,} tokens

## Methodology
Anthropic's SWE-bench methodology enhanced with Polydev:
1. **Polydev Consultation**: Every instance receives analysis from GPT-4, Claude, Gemini, Grok
2. **Insights Synthesis**: Multi-model perspectives combined for enhanced context
3. **Enhanced Generation**: Claude uses synthesized insights for patch generation
4. Extended thinking with 128K token budget
5. Default sampling parameters

## Results
- **Total Instances**: {stats['total']}
- **Patches Generated**: {stats['patches_generated']} ({100*stats['patches_generated']/max(1,stats['total']):.1f}%)
- **Polydev Calls**: {stats['polydev_calls']}
- **Claude Cost**: ${stats['total_cost']:.2f}
- **Polydev Cost**: ${stats['polydev_cost']:.2f}
- **Total Cost**: ${total_cost:.2f}
- **Total Tokens**: {stats['total_input_tokens'] + stats['total_output_tokens']:,}

## Run Details
- **Run ID**: {stats['run_id']}
- **Date**: {datetime.now().strftime("%Y-%m-%d")}
- **Duration**: {elapsed:.0f} minutes

## Submission
```bash
sb-cli submit swe-bench_verified test --predictions_path all_preds.jsonl --run_id {stats['run_id']}
```
"""

    readme_file = BASE_OUTPUT_DIR / "README.md"
    readme_file.write_text(readme)


def main():
    parser = argparse.ArgumentParser(description="SWE-bench Polydev-Enhanced Runner")
    parser.add_argument("--workers", type=int, default=10, help="Number of parallel workers")
    parser.add_argument("--test", type=int, help="Test mode: run on N samples only")
    parser.add_argument("--resume", action="store_true", help="Resume from previous run")
    parser.add_argument("--run-id", type=str, help="Custom run ID")
    args = parser.parse_args()

    # Setup
    run_id = args.run_id or f"polydev-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    stats["run_id"] = run_id

    print("="*80)
    print("SWE-BENCH POLYDEV-ENHANCED RUNNER")
    print("Multi-Model AI Consultation for Improved Results")
    print("="*80)
    print(f"  Run ID: {run_id}")
    print(f"  Model: {MODEL}")
    print(f"  Thinking Budget: {THINKING_TOKENS:,} tokens")
    print(f"  Workers: {args.workers}")
    print(f"  Polydev: ENABLED (every instance)")
    if args.test:
        print(f"  TEST MODE: {args.test} samples only")
    if args.resume:
        print(f"  RESUME MODE: Continuing from previous run")
    print("="*80)

    # Create output directories
    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(exist_ok=True)
    TRAJS_DIR.mkdir(exist_ok=True)
    POLYDEV_DIR.mkdir(exist_ok=True)

    # Load dataset
    print("\nLoading SWE-bench Verified dataset...")
    ds = load_dataset("princeton-nlp/SWE-bench_Verified", split="test")
    all_instances = list(ds)

    # Handle resume
    completed_instances = set()
    if args.resume:
        completed_instances = load_completed_instances()
        print(f"  Found {len(completed_instances)} already completed instances")

    # Filter instances
    instances = [inst for inst in all_instances if inst["instance_id"] not in completed_instances]

    # Test mode
    if args.test:
        instances = instances[:args.test]

    stats["total"] = len(instances)
    stats["start_time"] = time.time()

    print(f"\nInstances to process: {len(instances)}")
    print("Starting parallel execution with Polydev consultation...")
    print("="*80 + "\n")

    # Start progress reporter
    reporter = threading.Thread(target=progress_reporter, daemon=True)
    reporter.start()

    # Run in parallel
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = {executor.submit(run_instance, inst): inst for inst in instances}

        for future in as_completed(futures):
            result = future.result()

            # Save prediction (leaderboard format)
            with lock:
                with open(PREDICTIONS_FILE, "a") as f:
                    pred = {
                        "instance_id": result["instance_id"],
                        "model_name_or_path": result["model_name_or_path"],
                        "model_patch": result["model_patch"]
                    }
                    f.write(json.dumps(pred) + "\n")

                # Save metrics
                with open(METRICS_FILE, "a") as f:
                    metrics_entry = {
                        "instance_id": result["instance_id"],
                        "success": result["success"],
                        "metrics": result["metrics"],
                        "error": result.get("error"),
                        "timestamp": datetime.now().isoformat()
                    }
                    f.write(json.dumps(metrics_entry) + "\n")

            # Print status
            status = "✓" if result["success"] else "✗"
            pd_status = "P" if result["metrics"].get("polydev_success") else "p"
            err = f" [{result.get('error', '')[:25]}]" if result.get("error") else ""
            m = result["metrics"]
            patch_rate = 100 * stats["patches_generated"] / max(1, stats["completed"])
            print(f"[{stats['completed']}/{stats['total']}] {status}{pd_status} {result['instance_id'][:42]:<42} "
                  f"t:{m['turns']:>3} ${m['cost_usd']:.3f} {m['duration_sec']:>4.0f}s "
                  f"[{stats['patches_generated']} ({patch_rate:.0f}%)]{err}")

    # Final summary
    log_progress()

    # Save metadata and README
    try:
        save_metadata()
        save_readme()
    except Exception as e:
        print(f"Warning: Could not save metadata/readme: {e}")

    total_cost = stats["total_cost"] + stats["polydev_cost"]
    print(f"\n{'='*80}")
    print("POLYDEV-ENHANCED RUN COMPLETE!")
    print(f"{'='*80}")
    print(f"  Predictions: {PREDICTIONS_FILE}")
    print(f"  Metrics: {METRICS_FILE}")
    print(f"  Polydev Responses: {POLYDEV_DIR}")
    print(f"\n  Patches: {stats['patches_generated']}/{stats['total']} ({100*stats['patches_generated']/max(1,stats['total']):.1f}%)")
    print(f"  Polydev Calls: {stats['polydev_calls']}")
    print(f"  Claude Cost: ${stats['total_cost']:.4f}")
    print(f"  Polydev Cost: ${stats['polydev_cost']:.4f}")
    print(f"  Total Cost: ${total_cost:.4f}")
    print(f"\n  To submit to leaderboard:")
    print(f"  sb-cli submit swe-bench_verified test --predictions_path {PREDICTIONS_FILE} --run_id {run_id}")
    print("="*80)


if __name__ == "__main__":
    main()
